{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradeep333Singh/Pw_Assignments_DataScience/blob/main/Boosting_Assignment_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OXH8rXL1tbt"
      },
      "source": [
        "# **Boosting Techniques | Assignment Solutions**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fux3Wk2Y1tbu"
      },
      "source": [
        "## **Question 1**\n",
        "What is Boosting in Machine Learning? Explain how it improves weak learners.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4maYsB3O1tbv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Boosting** is an ensemble learning technique used to convert a set of weak learners (models that perform slightly better than random guessing) into a strong learner.\n",
        "\n",
        "**How it improves weak learners:**\n",
        "* **Sequential Learning:** Unlike Bagging (which trains in parallel), Boosting trains models sequentially. Each new model focuses on the errors made by the previous ones.\n",
        "* **Reweighting:** It assigns higher weights to data points that were misclassified by previous models, forcing the next learner to focus on these \"hard-to-classify\" examples.\n",
        "* **Bias Reduction:** By iteratively correcting errors, boosting effectively reduces the bias of the combined model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K39XTGXp1tbv"
      },
      "source": [
        "## **Question 2**\n",
        "What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT22tHRh1tbv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "| Feature | **AdaBoost (Adaptive Boosting)** | **Gradient Boosting** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Error Handling** | It identifies shortcomings by spotting **misclassified data points** and increasing their weights. | It identifies shortcomings by calculating the **residuals** (gradients) of the loss function (the difference between predicted and actual values). |\n",
        "| **Model Construction** | Each new tree is trained on a **re-weighted version** of the original dataset. | Each new tree is trained directly on the **residuals** (errors) of the previous tree. |\n",
        "| **Tree Depth** | Typically uses \"stumps\" (very short trees with depth=1). | Typically uses deeper trees (e.g., depth 4-8) compared to AdaBoost. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjB3EA2V1tbw"
      },
      "source": [
        "## **Question 3**\n",
        "How does regularization help in XGBoost?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCjHWAdO1tbw"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Regularization in XGBoost helps prevent **overfitting**, which is a common issue in boosting algorithms due to their aggressive nature of minimizing errors. XGBoost includes standard regularization parameters in its objective function:\n",
        "\n",
        "1.  **L1 Regularization (Lasso / Alpha):** Penalizes the absolute value of leaf weights. It can induce sparsity (pushing some weights to zero), which acts as a form of feature selection.\n",
        "2.  **L2 Regularization (Ridge / Lambda):** Penalizes the square of leaf weights. This keeps the weights small and stable, making the model less sensitive to individual data points.\n",
        "3.  **Gamma (Minimum Loss Reduction):** Specifies a minimum loss reduction required to make a further partition on a leaf node, effectively pruning the tree.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF8-9tFu1tbw"
      },
      "source": [
        "## **Question 4**\n",
        "Why is CatBoost considered efficient for handling categorical data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcPKLKn1tbw"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "CatBoost (Categorical Boosting) is specifically designed to handle categorical data efficiently without extensive pre-processing (like One-Hot Encoding) for the following reasons:\n",
        "\n",
        "* **Ordered Target Statistics:** Instead of standard Target Encoding (which can lead to target leakage), CatBoost uses a technique called \"Ordered Boosting.\" It calculates target statistics for a current data point using only the data points observed *before* it in a random permutation.\n",
        "* **Handling High Cardinality:** It can handle features with many categories automatically, reducing the dimensionality explosion that happens with One-Hot Encoding.\n",
        "* **Feature Combinations:** CatBoost automatically combines categorical features to create new interaction features during the tree-building process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7HhMEhp1tbx"
      },
      "source": [
        "## **Question 5**\n",
        "What are some real-world applications where boosting techniques are preferred over bagging methods?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRvKydYo1tbx"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Boosting is generally preferred over Bagging (like Random Forest) in scenarios where **high accuracy** is paramount and the data is clean (not overly noisy).\n",
        "\n",
        "1.  **Imbalanced Class Problems:** Applications like **Fraud Detection** or **Rare Disease Diagnosis** benefit from Boosting because it forces the model to focus on the minority class (the hard-to-classify examples).\n",
        "2.  **Search Ranking & Recommendations:** Web search engines (e.g., Google, Bing) and recommendation systems (e.g., Netflix) often use Gradient Boosting (LambdaMART) for ranking results.\n",
        "3.  **Kaggle/Data Science Competitions:** Boosting algorithms (XGBoost, LightGBM, CatBoost) are dominant in tabular data competitions due to their superior predictive performance.\n",
        "4.  **Credit Risk Scoring:** Financial institutions use boosting to predict loan defaults with high precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvoZPW_c1tbx"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW4DNYpM1tbx"
      },
      "source": [
        "## **Question 6**\n",
        "Write a Python program to:\n",
        "* Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "* Print the model accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsiWfxOL1tbx"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train the AdaBoost Classifier\n",
        "# Using default base estimator (Decision Tree Stump)\n",
        "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and Evaluate\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiahIzY31tby"
      },
      "source": [
        "## **Question 7**\n",
        "Write a Python program to:\n",
        "* Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "* Evaluate performance using R-squared score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE-m24tR1tby"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train the Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and Evaluate\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cCQkLfL1tby"
      },
      "source": [
        "## **Question 8**\n",
        "Write a Python program to:\n",
        "* Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "* Tune the learning rate using GridSearchCV\n",
        "* Print the best parameters and accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s982Lbpb1tby"
      },
      "outputs": [],
      "source": [
        "# Install XGBoost if not already installed\n",
        "!pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load Data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Initialize XGBoost Classifier\n",
        "# use_label_encoder=False and eval_metric='logloss' to avoid warnings\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# 3. Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# 4. Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n",
        "\n",
        "# 5. Train with Grid Search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print Best Parameters and Accuracy\n",
        "print(\"-\" * 30)\n",
        "print(f\"Best Learning Rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Optional: Test set accuracy with best model\n",
        "best_model = grid_search.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)\n",
        "print(f\"Test Set Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEe81Pgj1tby"
      },
      "source": [
        "## **Question 9**\n",
        "Write a Python program to:\n",
        "* Train a CatBoost Classifier\n",
        "* Plot the confusion matrix using seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c54hr36K1tby"
      },
      "outputs": [],
      "source": [
        "# Install CatBoost\n",
        "!pip install catboost\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load Data (Using Breast Cancer dataset as generic example)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train CatBoost Classifier\n",
        "# verbose=0 suppresses the training output logs\n",
        "cat_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=5, verbose=0, random_state=42)\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predict\n",
        "y_pred = cat_model.predict(X_test)\n",
        "\n",
        "# 4. Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2cR7BIy1tby"
      },
      "source": [
        "## **Question 10**\n",
        "You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CpNH4df1tbz"
      },
      "source": [
        "**Pipeline Description:**\n",
        "\n",
        "1.  **Data Preprocessing:**\n",
        "    * **Missing Values:** Use Median imputation for numerical features (robust to outliers) and 'Missing' or Mode imputation for categorical features.\n",
        "    * **Categorical Encoding:** Since we will likely use Boosting, we can use Target Encoding (if using XGBoost/LightGBM) or leave them as is if using CatBoost.\n",
        "    * **Imbalance Handling:** Apply SMOTE (Synthetic Minority Over-sampling Technique) or adjust `scale_pos_weight` (in XGBoost) to penalize errors on the minority class more heavily.\n",
        "\n",
        "2.  **Model Choice: CatBoost or XGBoost**\n",
        "    * **Selection:** I would choose **CatBoost** primarily because the dataset contains categorical features (demographics) and CatBoost handles these natively and efficiently without data leakage.\n",
        "\n",
        "3.  **Hyperparameter Tuning:**\n",
        "    * Tune `learning_rate`, `depth` (tree depth), and `l2_leaf_reg` (regularization).\n",
        "    * Crucially, tune the `class_weights` or `scale_pos_weight` parameter to handle the loan default imbalance.\n",
        "\n",
        "4.  **Evaluation Metrics:**\n",
        "    * **ROC-AUC:** To measure the model's ability to distinguish between defaulters and non-defaulters across thresholds.\n",
        "    * **Precision-Recall AUC (PR-AUC):** Since the positive class (default) is rare, PR-AUC is more informative than ROC-AUC.\n",
        "    * **F1-Score:** To balance precision and recall.\n",
        "\n",
        "5.  **Business Benefit:**\n",
        "    * The model allows the company to **minimize risk** by identifying high-risk applicants before approval.\n",
        "    * It allows for **Dynamic Pricing**: offering lower interest rates to low-risk customers and adjusting terms for higher-risk ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT4orcra1tbz"
      },
      "outputs": [],
      "source": [
        "# Pseudo-code / Skeleton Pipeline implementation\n",
        "# This code demonstrates the logic described above using a Pipeline structure.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- MOCK DATA CREATION (For demonstration purposes) ---\n",
        "# Creating a dummy dataframe to represent FinTech data\n",
        "df = pd.DataFrame({\n",
        "    'Income': np.random.randint(20000, 100000, 100),\n",
        "    'Credit_Score': np.random.randint(300, 850, 100),\n",
        "    'Employment_Type': np.random.choice(['Salaried', 'Self-Employed', 'Business'], 100),\n",
        "    'Loan_Default': np.random.choice([0, 1], 100, p=[0.9, 0.1]) # Imbalanced\n",
        "})\n",
        "# Introduce missing values\n",
        "df.loc[0:5, 'Income'] = np.nan\n",
        "\n",
        "X = df.drop('Loan_Default', axis=1)\n",
        "y = df['Loan_Default']\n",
        "\n",
        "# --- PIPELINE START ---\n",
        "\n",
        "# 1. Define Preprocessing for Numeric and Categorical columns\n",
        "numeric_features = ['Income', 'Credit_Score']\n",
        "categorical_features = ['Employment_Type']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 2. Define the Model (XGBoost with imbalance handling)\n",
        "# scale_pos_weight = count(negative) / count(positive) approx 9 in this dummy data\n",
        "clf = XGBClassifier(scale_pos_weight=9, eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# 3. Create the full Pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', clf)])\n",
        "\n",
        "# 4. Train and Evaluate\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Pipeline Steps Completed: Preprocessing -> Encoding -> XGBoost\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}